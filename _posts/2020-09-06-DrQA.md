---
layout: post
title: "DrQA学习笔记"
categories:
  - Edge Case
tags:
  - content
  - css
  - edge case
  - lists
  - markup
---

Reading Wikipedia to Answer Open-Domain Questions

url: https://arxiv.org/pdf/1704.00051.pdf

github: https://github.com/facebookresearch/DrQA

## **（一）摘要**

该文章以维基百科作为知识来源，建立了一个开放域问答系统DrQA，用于处理大规模机器阅读任务（MRS, machine reading at scale）。

模型包含两个部分，分别是**Document Retriever**和**Document Reader**，分别用于从广大的数据来源中提取与问题相关的文章，根据提取的文章找到问题的答案，完成阅读理解工作。

实验结果表明文章建立的两个部分的模型和其他已有的模型相比效果较好；**多任务学习**以及**远距离监督**的结合对于这一项任务而言十分有效。

## **（二）DrQA**
总体上，DrQA具有以下几个特点：
* 将bigram与TF-IDF结合，使用哈希降维，减少存储空间；
* 段落的字符编码考虑了词嵌入，词性，与问题相关的硬注意力以及软注意力；
* 通过远程监督为传统数据集扩充样本，结合多任务学习；
* 使用维基百科作为唯一数据来源，不需要预先划定段落重点；
# *1. Document Retriever*
模型概述：

对于相关文章提取部分，作者采用了经典的信息检索（非机器学习）思想来缩小搜索范围：分别计算问题和文章的bigram的TF-IDF向量，然后结合两个TF-IDF得到与问题最相关的五篇文章。

这种基于统计的做法可以保证检索速度。缺点在于完全基于统计的做法忽略了词与词之间的内在含义的关联性，且与Document Reader分离，无法进行端对端训练。

具体实现：

* 对语料单词进行清洗，包括去停词等过滤操作
* 统计所有的bigram，并对bigram做同样规则的清洗得到最终的bigram
* 将这些bigram进行murmur3 hashing得到每个bigram的id（如果哈系特征数目设置过小，可能会有两个不同bigram的id相同，文章用了特征数目为2^24，可以尽量避免这种哈希冲突）
* 根据TF-IDF公式计算每个bigram的IDF向量以及TF向量，将IDF乘以TF得到TF-IDF向量
* 将问题的TF-IDF向量与文章的TF-IDF向量相乘取最大(cosine similarity)的前五个的文章的索引，得到与问题最相关的5篇文章（因为TF-IDF是衡量一个词（或其他形式的元组）对一个文档的重要性，如果一些词既对于问题很重要，又对于文章很重要，那么就可以得出结论这个问题与这个文章的关联性很大）

# *2. Document Reader*
Notation:

Question *q* consisting of *l* tokens *{q<sub>1</sub>,...,q<sub>l</sub>}* 或者是 document *p* consists of *m* tokens *{p<sub>1</sub>,...,p<sub>m</sub>}*. 使用RNN apply to each paragraph in turn.

**Paragraph encoding**

1. 将token *p<sub>i</sub>* 转换成特征向量 $\widetilde{p_{i}}$
2. 将$\widetilde{p_{i}}$作为多层双向LSTM的输入，取各层的隐藏单元得到$p_{i}$,which 包含了token *p<sub>i</sub>* 附近的上下文信息 
  
  $p_{i}$ consist of the following part

  * *Word embddings* : $E(p_{i})$ 使用训练好的300维Glove词向量，保留绝大多数词向量，对出现频率最高的词进行fine-tune *(how, which, what)*
  * *Exact match* : 引入一个binary feature, 表示该单词是否对应问题中的某一个单词，是否是小写原始形式，是否是词根形式
  * *Token Features* : 描述词本身的属性，包括词性(POS)，命名体(NER),以及归一化过后的词频(TF)
  * *Aligned question embedding* : 用来描述paragraph中与question中每个单词对齐的embedding,$a_{i,j}$ attention score captures的similarity between $p_{i}$ and **each** question word $q_{j}$
  $$\frac{exp(\alpha(E(p_{i}))*\alpha(E(p_{i})))}{\sum_{j'} exp(\alpha(E(p_{i})) * \alpha(E(p_{j'})))}$$ 
  where $\alpha()$ is a dense layer with ReLU nonlinearity. 本特征不是判断是否完全一样，而是用相似度来衡量，相当于软注意力机制，exact match是硬注意力

**Paragraph encoding**
1. 将token *p<sub>i</sub>* 的word embedding $p_{i}$作为RNN的输入
2. 把hidden units按照重要程度加权拼接一个向量q. 权重 $b_{j}$ 按照
$$b_{j} = \frac{p(w*q_{j})} {\sum_{j'} exp(w*q_{j'})}$$
where **w** is a weight vector to learn

**Prediction**
把 paragraph vector $p_{1},p_{2}...p_{m}$ and the question vector $q$ 作为 input, 并分别 train two classifiers 以决定答案需要span的两端, 通过两个带有exp函数的现行网络分别计算每个字符成为开始和结束字符的概率.
$$P_{start}(i) \propto exp(p_{i}W_{s}q)$$
$$P_{end}(i) \propto exp(p_{i}W_{e}q)$$
*选择best span*: with start i和end i', 满足$i<=i'<=i+15$ 而且$P_{start}(i)*P_{start}(i')$最大

*lost function*: start_score与target_start的负对数似然函数加end_score与target_end的负对数似然函数得到最终的损失函数，进而可以使用反向传播来更新所有参数